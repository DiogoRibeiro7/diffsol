{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018bf0a9",
   "metadata": {},
   "source": [
    "# Neural ODE Classifier (Diffsol)\n",
    "\n",
    "This tutorial mirrors `examples/integration/neural_ode/train.py` but uses a bite-sized configuration\n",
    "that runs quickly on CPU. We:\n",
    "\n",
    "1. Map Fashion-MNIST images to a scalar rate parameter.\n",
    "2. Integrate a 1-D logistic ODE with `DiffsolModule`.\n",
    "3. Train the classifier end-to-end and visualise the loss curve.\n",
    "4. Check gradients with the utilities from `diffsol_pytorch.testing`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from diffsol_pytorch import DiffsolModule, device, testing\n",
    "from helpers import (\n",
    "    describe_device,\n",
    "    gpu_section_mode,\n",
    "    load_fashion_mnist_subset,\n",
    "    preferred_device,\n",
    "    save_cached_json,\n",
    "    seed_everything,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGISTIC_CODE = (\"\n",
    "in = [k]\n",
    "\"\n",
    "    \"k { 0.7 }\n",
    "\"\n",
    "    \"u { u = 1.0, }\n",
    "\"\n",
    "    \"F { -k * u, }\n",
    "\")\n",
    "\n",
    "TIMES = torch.linspace(0.0, 1.0, 41, dtype=torch.float64)\n",
    "TIMES_LIST = TIMES.tolist()\n",
    "GRAD_OUT_FINAL = [0.0] * (len(TIMES_LIST) - 1) + [1.0]\n",
    "MODULE = DiffsolModule(LOGISTIC_CODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffsolLogistic(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, rates: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = []\n",
    "        for rate in rates.detach().cpu().tolist():\n",
    "            _, _, flat = MODULE.solve_dense([rate], TIMES_LIST)\n",
    "            outputs.append(flat[-1])\n",
    "        u_final = torch.tensor(outputs, dtype=rates.dtype, device=rates.device)\n",
    "        probs = 1.0 - u_final\n",
    "        ctx.save_for_backward(rates.detach())\n",
    "        return probs.clamp(1e-6, 1 - 1e-6)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        (rates,) = ctx.saved_tensors\n",
    "        grads = []\n",
    "        grad_out = np.array(GRAD_OUT_FINAL, dtype=np.float64)\n",
    "        for rate, upstream in zip(rates.tolist(), grad_output.detach().cpu().tolist()):\n",
    "            g = testing.reverse_mode_gradients(LOGISTIC_CODE, [rate], TIMES_LIST, grad_out)[0]\n",
    "            grads.append(-g * upstream)\n",
    "        grad_tensor = torch.tensor(grads, dtype=grad_output.dtype, device=grad_output.device)\n",
    "        return grad_tensor\n",
    "\n",
    "\n",
    "class DiffsolClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc = nn.Linear(16 * 7 * 7, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.encoder(x)\n",
    "        rates = torch.nn.functional.softplus(self.fc(z.flatten(1))) + 1e-3\n",
    "        probs = DiffsolLogistic.apply(rates.squeeze(1))\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f675a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "loader = load_fashion_mnist_subset(samples=128, batch_size=32)\n",
    "device_target = preferred_device()\n",
    "print(f\"Using device: {describe_device(device_target)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device_target.type != 'cuda':\n",
    "    print('CUDA not available; running training on CPU. Enable a GPU runtime to compare performance.')\n",
    "else:\n",
    "    print(f'CUDA available on {torch.cuda.get_device_name(0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877787b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffsolClassifier().to(device_target)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "loss_history = []\n",
    "\n",
    "model.train()\n",
    "for _ in range(1):\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device_target, dtype=torch.float32)\n",
    "        targets = (labels % 2 == 0).float().to(device_target)\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "loss_history[:5], len(loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(loss_history, label=\"BCELoss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training curve (1 epoch, 128 samples)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device_target, dtype=torch.float32)\n",
    "            targets = (labels % 2 == 0).to(device_target)\n",
    "            preds = model(images)\n",
    "            predicted = (preds > 0.5).long()\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    model.train()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "accuracy = evaluate(model, loader)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode, cached_metrics = gpu_section_mode(\n",
    "    \"Neural ODE GPU benchmark\", cache_key=\"neural_ode_gpu_metrics.json\"\n",
    ")\n",
    "if mode == \"run\":\n",
    "    benchmark_device = preferred_device()\n",
    "    batch = next(iter(loader))\n",
    "    images = batch[0].to(benchmark_device, dtype=torch.float32)\n",
    "    model_eval = model.to(benchmark_device).eval()\n",
    "    if benchmark_device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        _ = model_eval(images)\n",
    "        if benchmark_device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "    latency_ms = (time.perf_counter() - start) * 1_000.0\n",
    "    metrics = {\n",
    "        \"device\": describe_device(benchmark_device),\n",
    "        \"batch_latency_ms\": round(latency_ms, 3),\n",
    "        \"batch_size\": int(images.shape[0]),\n",
    "    }\n",
    "    save_cached_json(\"neural_ode_gpu_metrics.json\", metrics)\n",
    "    model.to(device_target).train()\n",
    "elif mode == \"cache\":\n",
    "    metrics = cached_metrics\n",
    "else:\n",
    "    metrics = {\n",
    "        \"device\": \"cpu\",\n",
    "        \"note\": \"GPU benchmark skipped; set NB_FORCE_GPU=1 to require an accelerator.\",\n",
    "    }\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(solution: np.ndarray):\n",
    "    grad = np.zeros_like(solution)\n",
    "    grad[0, -1] = 1.0\n",
    "    return float(solution[0, -1]), grad\n",
    "\n",
    "results = testing.check_gradients(LOGISTIC_CODE, [0.7], TIMES_LIST, loss_fn)\n",
    "results[\"finite_difference\"], results[\"reverse_mode\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}